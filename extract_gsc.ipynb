{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extract-gsc.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOvH2DyYSFivp6VyXDq82zJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py5nnJYKRsNJ"
      },
      "source": [
        "All credits of this scripts goes to: Jean. Thanks for sharing this awesome script with us!! \n",
        "\n",
        "https://www.jcchouinard.com/get-all-keywords-with-search-console-api/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_wBriEhyWZq"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import date, timedelta\n",
        "import httplib2\n",
        "from googleapiclient.discovery import build\n",
        "from oauth2client.client import OAuth2WebServerFlow\n",
        "from collections import defaultdict\n",
        "from dateutil import relativedelta\n",
        "import argparse\n",
        "from oauth2client import client\n",
        "from oauth2client import file\n",
        "from oauth2client import tools\n",
        "import re\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "#from google.colab import drive # << only activate when just running this Notebook, otherwise don't need to\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xad8RCfpyjQJ"
      },
      "source": [
        "site = 'https://www.bcc.nl/' # << What's your domain?\n",
        "num_days = 8 # << How many days you wanna track?\n",
        "creds = '/content/drive/MyDrive/Colab Notebooks/SEO_reporting/credentials/client-secret.json' # << Put here the client-secret.json file\n",
        "output = 'gsc_data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5a1NEdXylmz"
      },
      "source": [
        "# Get Domain Name to Create a Project\n",
        "def get_domain_name(start_url):\n",
        "    domain_name = '{uri.netloc}'.format(uri=urlparse(start_url))  # Get Domain Name To Name Project\n",
        "    domain_name = domain_name.replace('.','_')\n",
        "    return domain_name\n",
        " \n",
        "# Create a project Directory for this website\n",
        "def create_project(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        print('Create project: '+ directory)\n",
        "        os.makedirs(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ThrNqHyquC"
      },
      "source": [
        "def authorize_creds(creds):\n",
        "    # Variable parameter that controls the set of resources that the access token permits.\n",
        "    SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly'] \n",
        " \n",
        "    # Path to client_secrets.json file\n",
        "    CLIENT_SECRETS_PATH = creds\n",
        " \n",
        "    # Create a parser to be able to open browser for Authorization\n",
        "    parser = argparse.ArgumentParser(\n",
        "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
        "        parents=[tools.argparser])\n",
        "    flags = parser.parse_args([])\n",
        " \n",
        "    flow = client.flow_from_clientsecrets(\n",
        "        CLIENT_SECRETS_PATH, scope = SCOPES,\n",
        "        message = tools.message_if_missing(CLIENT_SECRETS_PATH))\n",
        " \n",
        "    # Prepare credentials and authorize HTTP\n",
        "    # If they exist, get them from the storage object\n",
        "    # credentials will get written back to a file.\n",
        "    storage = file.Storage('authorizedcreds.dat')\n",
        "    credentials = storage.get()\n",
        " \n",
        "    # If authenticated credentials don't exist, open Browser to authenticate\n",
        "    if credentials is None or credentials.invalid:\n",
        "        credentials = tools.run_flow(flow, storage, flags)\n",
        "    http = credentials.authorize(http=httplib2.Http())\n",
        "    webmasters_service = build('webmasters', 'v3', http=http)\n",
        "    return webmasters_service\n",
        " \n",
        "# Create Function to execute your API Request\n",
        "def execute_request(service, property_uri, request):\n",
        "    return service.searchanalytics().query(siteUrl=property_uri, body=request).execute()\n",
        " \n",
        "# Create function to write to CSV\n",
        "def write_to_csv(data,filename):\n",
        "    if not os.path.isfile(filename):\n",
        "        data.to_csv(filename)\n",
        "    else: # else it exists so append without writing the header\n",
        "        data.to_csv(filename, mode='a', header=False)\n",
        "\n",
        "# Read CSV if it exists to find dates that have already been processed.\n",
        "def get_dates_from_csv(path):\n",
        "    if os.path.isfile(path):\n",
        "        data = pd.read_csv(path)\n",
        "        data = pd.Series(data['date'].unique())\n",
        "        return data\n",
        "    else:\n",
        "        pass \n",
        "    \n",
        "# Create function to extract all the data\n",
        "def extract_data(site,creds,num_days,output):\n",
        "    domain_name = get_domain_name(site)\n",
        "    create_project(domain_name)\n",
        "    full_path = domain_name + '/' + output\n",
        "    current_dates = get_dates_from_csv(full_path)\n",
        " \n",
        "    webmasters_service = authorize_creds(creds)\n",
        " \n",
        "    # Set up Dates\n",
        "    end_date = datetime.date.today() - relativedelta.relativedelta(days=3)\n",
        "    start_date = end_date - relativedelta.relativedelta(days=num_days)\n",
        "    delta = datetime.timedelta(days=1) # This will let us loop one day at the time\n",
        "    scDict = defaultdict(list)\n",
        " \n",
        "    while start_date <= end_date:\n",
        "        if current_dates is not None and current_dates.str.contains(datetime.datetime.strftime(start_date,'%Y-%m-%d')).any():\n",
        "            #print('Existing Date: %s' % start_date)\n",
        "            start_date += delta     \n",
        "        else:\n",
        "            #print('Start date at beginning: %s' % start_date)\n",
        " \n",
        "            maxRows = 25000 # Maximum 25K per call \n",
        "            numRows = 0     # Start at Row Zero\n",
        "            status = ''     # Initialize status of extraction\n",
        " \n",
        " \n",
        "            while (status != 'Finished') : # Test with i < 10 just to see how long the task will take to process.\n",
        "                request = {\n",
        "                    'startDate': datetime.datetime.strftime(start_date,'%Y-%m-%d'),\n",
        "                    'endDate': datetime.datetime.strftime(start_date,'%Y-%m-%d'),\n",
        "                    'dimensions': ['date','page','query'],\n",
        "                    'rowLimit': maxRows, \n",
        "                    'startRow': numRows\n",
        "                }\n",
        " \n",
        "                response = execute_request(webmasters_service, site, request)\n",
        " \n",
        "                try:\n",
        "                #Process the response\n",
        "                    for row in response['rows']:\n",
        "                        scDict['date'].append(row['keys'][0] or 0)    \n",
        "                        scDict['page'].append(row['keys'][1] or 0)\n",
        "                        scDict['query'].append(row['keys'][2] or 0)\n",
        "                        scDict['clicks'].append(row['clicks'] or 0)\n",
        "                        scDict['ctr'].append(row['ctr'] or 0)\n",
        "                        scDict['impressions'].append(row['impressions'] or 0)\n",
        "                        scDict['position'].append(row['position'] or 0)\n",
        "                    print('successful at %i' % numRows)\n",
        " \n",
        "                except:\n",
        "                    print('error occurred at %i' % numRows)\n",
        " \n",
        "                #Add response to dataframe \n",
        "                df = pd.DataFrame(data = scDict)\n",
        "                df['clicks'] = df['clicks'].astype('int')\n",
        "                df['ctr'] = df['ctr']*100\n",
        "                df['impressions'] = df['impressions'].astype('int')\n",
        "                df['position'] = df['position'].round(2)\n",
        " \n",
        "                #print('Numrows at the start of loop: %i' % numRows)\n",
        "                try: \n",
        "                    numRows = numRows + len(response['rows'])\n",
        "                except:\n",
        "                    status = 'Finished'\n",
        "                print('Numrows at the end of loop: %i' % numRows)\n",
        "                if numRows % maxRows != 0:\n",
        "                    status = 'Finished'\n",
        "         \n",
        "            start_date += delta  \n",
        "            print('Start date at end: %s' % start_date) \n",
        "            write_to_csv(df,full_path)\n",
        "    return df\n",
        " \n",
        "df = extract_data(site,creds,num_days,output)\n",
        "df.sort_values('clicks',ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}